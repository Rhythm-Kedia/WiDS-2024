{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNpQdZ29Xbrb"
   },
   "source": [
    "---\n",
    "AND Gate implementation perceptron\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bv-TYMHJK6lk",
    "outputId": "3081688d-687e-404a-b895-4dddc31d4d63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Perceptron for AND Gate:\n",
      "Input: [0 0], Prediction: 0\n",
      "Input: [0 1], Prediction: 0\n",
      "Input: [1 0], Prediction: 0\n",
      "Input: [1 1], Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AND_NN:\n",
    "    def __init__(self, learning_rate=0.1, num_itr=100):\n",
    "        self.weights = np.random.randn(3)  # Include bias weight\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_itr = num_itr\n",
    "\n",
    "    def activation_function(self, z):\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Ensure input is 2D and add bias column\n",
    "        X = np.atleast_2d(X)\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        Z = np.dot(X_with_bias, self.weights)\n",
    "        return self.activation_function(Z)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Ensure input is 2D and add bias column\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        for _ in range(self.num_itr):\n",
    "            predictions = self.predict(X)\n",
    "            errors = y - predictions\n",
    "            self.weights += self.learning_rate * np.dot(errors, X_with_bias)\n",
    "\n",
    "# Input and output for AND gate\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y = np.array([0, 0, 0, 1])  # Outputs\n",
    "\n",
    "and_gate = AND_NN()\n",
    "and_gate.train(X, y)\n",
    "\n",
    "# Test the perceptron\n",
    "print(\"Testing Perceptron for AND Gate:\")\n",
    "predictions = and_gate.predict(X)\n",
    "for inputs, prediction in zip(X, predictions):\n",
    "    print(f\"Input: {inputs}, Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbXSz7WEXXPo"
   },
   "source": [
    "---\n",
    "This is a XOR Gate implementation without using hidden layers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41n2_XOf7LYJ",
    "outputId": "717572e5-70d1-45d1-bfc7-4b171bac2fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected outputs: [0 1 1 0]\n",
      "Predicted outputs: [0 0 0 0]\n",
      "The single-layer XOR fails to learn the XOR function.\n"
     ]
    }
   ],
   "source": [
    "class XOR_perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1, num_itr=100):\n",
    "        self.weights = np.random.rand(input_size + 1) # Include weights for bias\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_itr = num_itr\n",
    "\n",
    "    def activation_function(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.activation_function(np.dot(X, self.weights))\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Train the single-layer network using the given dataset.\n",
    "        for _ in range(self.num_itr):\n",
    "            predictions = self.predict(X)\n",
    "            errors = y - predictions\n",
    "            self.weights += self.learning_rate * np.dot(errors, X)\n",
    "\n",
    "    def test(self, X):\n",
    "        # Test the network on given inputs.\n",
    "        return self.predict(X)\n",
    "\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "outputs = np.array([0, 1, 1, 0])  # XOR outputs\n",
    "# Add bias as an extra input (column of ones)\n",
    "inputs_with_bias = np.hstack((inputs, np.ones((inputs.shape[0], 1))))\n",
    "\n",
    "# Train the single-layer network\n",
    "xor_nn = XOR_perceptron(input_size=2)\n",
    "xor_nn.train(inputs_with_bias, outputs)\n",
    "\n",
    "# Test the network\n",
    "predicted_outputs = xor_nn.test(inputs_with_bias)\n",
    "\n",
    "# Print results\n",
    "print(\"Expected outputs:\", outputs)\n",
    "print(\"Predicted outputs:\", predicted_outputs)\n",
    "\n",
    "# Check if XOR was learned\n",
    "if np.array_equal(predicted_outputs, outputs):\n",
    "    print(\"The single-layer XOR learned the XOR function!\")\n",
    "else:\n",
    "    print(\"The single-layer XOR fails to learn the XOR function.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIIVmZrt76Ey"
   },
   "source": [
    "---\n",
    "Observation - **`No matter how many times the XOR_perceptron is trained, without a hidden layer it is unable to produce the required output.`**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYiEP4O7cF1X"
   },
   "source": [
    "---\n",
    "This is a XOR Gate implementation using a single hidden layer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mBCu-mOqaNpw",
    "outputId": "a023e9ae-b820-45fb-e736-753340f05b5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Predictions : [0. 1. 1. 0.]\n",
      "Training Time: 3.4659087657928467 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "class XOR_NN:\n",
    "    def __init__(self):\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        # 3 because it worked quite well\n",
    "\n",
    "        # Initialize weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3)\n",
    "        return yHat\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def sigmoidPrime(self, z):\n",
    "        # Derivative of the sigmoid activation function.\n",
    "        return np.exp(-z) / ((1 + np.exp(-z)) ** 2)\n",
    "\n",
    "    def costFunction(self, X, y):\n",
    "        # Compute the cost function.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5 * np.sum((y - self.yHat) ** 2)\n",
    "        return J\n",
    "\n",
    "    def costFunctionPrime(self, X, y):\n",
    "        # Compute the gradients of the cost function w.r.t. W1 and W2.\n",
    "        self.yHat = self.forward(X)\n",
    "        delta3 = np.multiply(-(y - self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)  # Gradient for W2\n",
    "        delta2 = np.dot(delta3, self.W2.T) * self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  # Gradient for W1\n",
    "\n",
    "        return dJdW1, dJdW2\n",
    "\n",
    "    def getParams(self):\n",
    "        # Get W1 and W2 parameters rolled into a single vector.\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "\n",
    "    def setParams(self, params):\n",
    "        # Set W1 and W2 using a single parameter vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize, self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize * self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "\n",
    "    def computeGradients(self, X, y):\n",
    "        # Compute the gradients of the cost function.\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))\n",
    "\n",
    "    def gradientDescent(self, X, y, num_itr, learningRate):\n",
    "        # Perform gradient descent to minimize the cost function.\n",
    "        for itr in range(num_itr):\n",
    "            gradients = self.computeGradients(X, y)\n",
    "            self.setParams(self.getParams() - learningRate * gradients)  # Update parameters using gradients\n",
    "            # Check how the cost changes with iterations\n",
    "            # if itr % 1000 == 0:\n",
    "            #     cost = self.costFunction(X, y)\n",
    "            #     print(f\"Iteration {itr}, Cost: {cost}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions based on the trained model.\n",
    "        return np.round(self.forward(X))\n",
    "\n",
    "# Training Data\n",
    "X = np.array([[0, 0],[0, 1],[1, 0],[1, 1]])\n",
    "Y = np.array([[0],[1],[1],[0]])\n",
    "# Set parameters and training configuration\n",
    "xor_nn = XOR_NN()\n",
    "num_itr = 100000\n",
    "learningRate = 0.1\n",
    "# Train the Model\n",
    "start_time = time.time()\n",
    "xor_nn.gradientDescent(X, Y, num_itr, learningRate)\n",
    "end_time = time.time()\n",
    "\n",
    "# Output the results\n",
    "predictions = xor_nn.predict(X)\n",
    "print(\"\\nFinal Predictions :\",predictions.flatten())\n",
    "print(f\"Training Time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcVjGhfDWt5G"
   },
   "source": [
    "\n",
    "---\n",
    "Implementation of Full Adder using XOR and AND gates\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sq_oaAhtSjh1",
    "outputId": "30050a56-1217-464a-a3ad-e4ee235590d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Full Adder:\n",
      " A  B  Cin  Sum  Carry\n",
      " 0  0    0    0      0\n",
      " 0  0    1    1      0\n",
      " 0  1    0    1      0\n",
      " 0  1    1    0      1\n",
      " 1  0    0    1      0\n",
      " 1  0    1    0      1\n",
      " 1  1    0    0      1\n",
      " 1  1    1    1      1\n"
     ]
    }
   ],
   "source": [
    "class FullAdderNN:\n",
    "    def __init__(self):\n",
    "        # Initialize XOR and AND neural networks\n",
    "        self.xor_nn = XOR_NN()  # XOR gate model\n",
    "        self.and_nn = AND_NN()  # AND gate model\n",
    "\n",
    "        # Train XOR for Sum calculation\n",
    "        X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Training for XOR gate\n",
    "        Y_xor = np.array([[0], [1], [1], [0]])  # Expected output for XOR\n",
    "        self.xor_nn.gradientDescent(X_xor, Y_xor, num_itr=100000, learningRate=0.1)\n",
    "\n",
    "        # Train AND for intermediate carries\n",
    "        X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Training for AND gate\n",
    "        Y_and = np.array([0, 0, 0, 1])  # Expected output for AND\n",
    "        self.and_nn.train(X_and, Y_and)\n",
    "\n",
    "    def predict(self, A, B, Cin):\n",
    "        A = np.array(A)\n",
    "        B = np.array(B)\n",
    "        Cin = np.array(Cin)\n",
    "        xor_ab = self.xor_nn.predict(np.column_stack((A, B)))\n",
    "\n",
    "        # Calculate final Sum = XOR(XOR(A, B), Cin)\n",
    "        sum_output = self.xor_nn.predict(np.column_stack((xor_ab, Cin)))\n",
    "\n",
    "        # Calculate final_carry = XOR(AND(A,B),AND(XOR(A,B),Cin))\n",
    "        and_ab = self.and_nn.predict(np.column_stack((A, B)))\n",
    "        and_cin_ab = self.and_nn.predict(np.column_stack((xor_ab, Cin)))\n",
    "        final_carry = self.xor_nn.predict(np.column_stack((and_ab, and_cin_ab)))\n",
    "\n",
    "        return sum_output.astype(int), final_carry.astype(int)\n",
    "\n",
    "full_adder_nn = FullAdderNN()\n",
    "\n",
    "# Test Full Adder with arrays\n",
    "A = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "B = np.array([0, 0, 1, 1, 0, 0, 1, 1])\n",
    "Cin = np.array([0, 1, 0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Get sum and carry\n",
    "sum_output, final_carry = full_adder_nn.predict(A, B, Cin)\n",
    "\n",
    "# Output the results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'A': A,'B': B,'Cin': Cin,'Sum': sum_output.flatten(),'Carry': final_carry.flatten()})\n",
    "df.index = [''] * len(df)  # Reset index to remove the first column\n",
    "print(\"Testing Full Adder:\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wn32AoG0d0yy"
   },
   "source": [
    "---\n",
    "Combining the adders into a ripple carry adder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMcgi4axXlTl",
    "outputId": "1474e21e-186c-47fb-9080-17b68dc75067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ripple Carry Adder Results:\n",
      "    A  B  Sum\n",
      "Carry ->    1\n",
      "    0  1    0\n",
      "    0  1    0\n",
      "    0  1    0\n",
      "    0  1    0\n",
      "    1  0    0\n",
      "    1  0    0\n",
      "    1  1    1\n",
      "    1  1    0\n",
      "Final Carry-Out: 1\n",
      "Sum in Decimal: 258\n"
     ]
    }
   ],
   "source": [
    "class RippleCarryAdderNN:\n",
    "    def __init__(self, full_adder_nn):\n",
    "        self.full_adder_nn = full_adder_nn\n",
    "\n",
    "    def predict(self, A, B, Cin=0):\n",
    "        A = np.array(A)\n",
    "        B = np.array(B)\n",
    "        n = len(A)\n",
    "        assert len(B) == n, \"A and B must have the same length\"\n",
    "\n",
    "        sum_output = np.zeros(n)\n",
    "        carry = Cin\n",
    "        for i in range(n - 1, -1, -1):\n",
    "            sum_bit, carry = self.full_adder_nn.predict([A[i]], [B[i]], [carry])\n",
    "            sum_output[i] = sum_bit.flatten()[0]\n",
    "            carry = carry.flatten()[0]\n",
    "        return sum_output, carry\n",
    "\n",
    "# Testing the RippleCarryAdderNN:\n",
    "full_adder_nn = FullAdderNN()\n",
    "ripple_carry_adder_nn = RippleCarryAdderNN(full_adder_nn)\n",
    "\n",
    "# Ensure that the arrays have the same length (int this case 8)\n",
    "A = [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "B = [1, 1, 1, 1, 0, 0, 1, 1]\n",
    "Cin = 0\n",
    "\n",
    "# Results\n",
    "sum_output, final_carry = ripple_carry_adder_nn.predict(A, B, Cin)\n",
    "sum_with_carry = np.insert(sum_output.flatten().astype(int), 0, int(final_carry))\n",
    "df = pd.DataFrame({'A': ['Carry']+A,'B': ['->']+B,'Sum': sum_with_carry,})\n",
    "df.index = [''] * len(df)\n",
    "\n",
    "print(\"Ripple Carry Adder Results:\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"Final Carry-Out: {int(final_carry)}\")\n",
    "\n",
    "decimal_sum = int(str(final_carry)+''.join(sum_output.astype(int).astype(str)), 2)\n",
    "print(f\"Sum in Decimal: {decimal_sum}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WNpQdZ29Xbrb",
    "dbXSz7WEXXPo",
    "NYiEP4O7cF1X"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
