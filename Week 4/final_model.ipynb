{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_rgmpc0P7iK"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, LayerNormalization, Dropout\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yp2I1PiaP7iK"
   },
   "outputs": [],
   "source": [
    "with open('training_data.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5z_l89zVP7iM",
    "outputId": "28215d8b-93a2-4bda-862d-9f5791f42c36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwXQDaRwP7iM",
    "outputId": "7f4eabc0-02f5-4add-8a7e-a0401bfdf715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "characters=list(set(list(data)))\n",
    "print(len(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XI9DbUMqP7iM"
   },
   "outputs": [],
   "source": [
    "character_to_integer_encoding={}\n",
    "integer_to_character_encoding={}\n",
    "for i in range(len(characters)):\n",
    "    character_to_integer_encoding[characters[i]]=i+1\n",
    "    integer_to_character_encoding[i+1]=characters[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E0Qyx0q1P7iM"
   },
   "outputs": [],
   "source": [
    "def encode(string):\n",
    "    global character_to_integer_encoding\n",
    "    return [character_to_integer_encoding[char] for char in string]\n",
    "\n",
    "def decode(lst):\n",
    "    global integer_to_character_encoding\n",
    "    return ''.join([integer_to_character_encoding[i] for i in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t_tvSz7P7iN"
   },
   "outputs": [],
   "source": [
    "input_data=encode(data)\n",
    "train_data=input_data[:int(0.9*len(input_data))]\n",
    "test_data=input_data[int(0.9*len(input_data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgG1LZZKP7iO"
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "block_size=128\n",
    "num_heads=16 # Experiment with other values if you want\n",
    "num_transformer_blocks = 4\n",
    "input_vocab_size=len(characters)+1\n",
    "feed_forward_dim = 256 # I am using the same dimensions for the embedding as well. This may be too high of a dimension, given that there are only 65 characters and 128 positions per block, but it will take a lot of time to test alternate parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpPchvsGP7iO"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Implementing the Multihead attention layer was something I tried,\n",
    "but ultimately it gave slower and worse results than calling layers.MultiHeadAttention\n",
    "(ig the people at tensorflow have put some effort into optimization).\n",
    "You can try modifying the code in this cell and using it instead of calling the inbuilt class\n",
    "'''\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, model_dimension):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.model_dimension = model_dimension\n",
    "        assert model_dimension % num_heads == 0\n",
    "\n",
    "        self.depth = model_dimension // num_heads\n",
    "        self.query_space_projector = Dense(model_dimension)\n",
    "        self.key_space_projector = Dense(model_dimension)\n",
    "        self.value_space_projector = Dense(model_dimension)\n",
    "        self.dense = Dense(model_dimension)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # Shape: (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src):\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j\n",
    "        mask = tf.cast(m, tf.bool)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])  # Shape: (1, n_dest, n_src)\n",
    "        mask = tf.tile(mask, [batch_size, 1, 1])  # Shape: (batch_size, n_dest, n_src)\n",
    "        mask = mask[:, tf.newaxis, :, :]  # Shape: (batch_size, 1, n_dest, n_src)\n",
    "        mask = tf.tile(mask, [1, self.num_heads, 1, 1])  # Shape: (batch_size, num_heads, n_dest, n_src)\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        q = self.query_space_projector(inputs)\n",
    "        k = self.key_space_projector(inputs)\n",
    "        v = self.value_space_projector(inputs)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
    "        k = self.split_heads(k, batch_size)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
    "        v = self.split_heads(v, batch_size)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        mask = self.causal_attention_mask(batch_size, tf.shape(inputs)[1], tf.shape(inputs)[1])\n",
    "\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "        attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # Shape: (batch_size, seq_len, num_heads, depth)\n",
    "\n",
    "        attention = tf.reshape(attention, (batch_size, -1, self.model_dimension))  # Shape: (batch_size, seq_len, model_dimension)\n",
    "\n",
    "        output = self.dense(attention)\n",
    "        return output\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # Shape: (batch_size, num_heads, seq_len, depth)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOyfpNKOP7iP"
   },
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src):\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, tf.bool)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    return tf.tile(mask, [batch_size, 1, 1])\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        # Give code for an attention layer, feedforward layers, and normalization layers. The attention layer is first, then normalization and dropout, then forward the data passed through a non-linear function, and call the dropout layer again\n",
    "        ###\n",
    "        # Insert code here\n",
    "        self.attention = MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = Sequential([\n",
    "            Dense(ff_dim, activation='relu'),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.normalization_layer_1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.normalization_layer_2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        block_size = input_shape[1]\n",
    "        attention_output = self.attention(inputs)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.normalization_layer_1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.normalization_layer_2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZF6AnvtP7iP"
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_embedding = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_embedding(positions)\n",
    "        x = self.token_embedding(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd6b2Xo1P7iP"
   },
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks):\n",
    "        super().__init__()\n",
    "        self.inputs = Input(shape=(maxlen,), dtype=tf.int32)\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, feed_forward_dim) for _ in range(num_transformer_blocks)]\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for i in range(self.num_transformer_blocks):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "        output = self.dense(x)\n",
    "        return output\n",
    "'''Above, we have a subclass-based representation of the model, and below, a functional API-based representation\n",
    "The functional API learns much faster and more efficiently, because apparently tensorflow has a bunch of optimizations\n",
    "for static graphs which are known to it before observing the data (https://www.tensorflow.org/guide/function, functional APIs make use of this paradigm by default)\n",
    "Secondly, the for loop in the call() function cannot be optimized in the Subclass API, but it is replaced by nodes in a graph in the functional API call,\n",
    "This avoids having to shuttle between executing the fast code the people behind tensorflow have developed and a slower python for loop.\n",
    "'''\n",
    "\n",
    "def get_transformer_model(\n",
    "    maxlen,\n",
    "    vocab_size,\n",
    "    embed_dim,\n",
    "    num_heads,\n",
    "    feed_forward_dim,\n",
    "    num_transformer_blocks=1\n",
    "):\n",
    "    inputs = Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "        x = transformer_block(x)\n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    model = Model(inputs=inputs, outputs=[outputs]) # This is a functional API-based representation of a tf model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Frq5Yc3oP7iP"
   },
   "outputs": [],
   "source": [
    "model = get_transformer_model(block_size, input_vocab_size, feed_forward_dim, num_heads, feed_forward_dim, num_transformer_blocks)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\n",
    "    \"adam\",\n",
    "    loss=[loss_fn],\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EaF1Lx6P7iP"
   },
   "outputs": [],
   "source": [
    "inputs = [train_data[i:i+block_size] for i in range(0, len(train_data)-block_size-1)]\n",
    "targets = [train_data[i+1:i+block_size+1] for i in range(0, len(train_data)-block_size-1)]\n",
    "'''\n",
    "Insert code here to preprocess the input data and the target data to send it to the model.\n",
    "'''\n",
    "inputs = np.array(inputs, dtype=np.int32)\n",
    "targets = np.array(targets, dtype=np.int32)\n",
    "\n",
    "dataset= tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "dataset = dataset.shuffle(10000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "iRlmiX7_P7iP",
    "outputId": "af8d31f8-ee00-4c02-94f9-ec48b68e8919"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">395,776</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_1                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">395,776</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_2                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">395,776</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_3                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">395,776</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,705</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │          \u001b[38;5;34m49,408\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m395,776\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_1                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m395,776\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_2                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m395,776\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ transformer_block_3                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m395,776\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)                   │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m65\u001b[0m)             │          \u001b[38;5;34m16,705\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,649,217</span> (6.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,649,217\u001b[0m (6.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,649,217</span> (6.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,649,217\u001b[0m (6.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvUfehoJP7iQ",
    "outputId": "454beced-9c61-4025-ea07-0521387b98e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1036s\u001b[0m 32ms/step - accuracy: 0.9620 - loss: 0.1437\n",
      "Epoch 2/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1026s\u001b[0m 32ms/step - accuracy: 0.9937 - loss: 0.0221\n",
      "Epoch 3/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1041s\u001b[0m 32ms/step - accuracy: 0.9951 - loss: 0.0171\n",
      "Epoch 4/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1045s\u001b[0m 32ms/step - accuracy: 0.9954 - loss: 0.0160\n",
      "Epoch 5/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1017s\u001b[0m 32ms/step - accuracy: 0.9955 - loss: 0.0153\n",
      "Epoch 6/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1045s\u001b[0m 33ms/step - accuracy: 0.9956 - loss: 0.0150\n",
      "Epoch 7/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1031s\u001b[0m 32ms/step - accuracy: 0.9956 - loss: 0.0153\n",
      "Epoch 8/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1016s\u001b[0m 32ms/step - accuracy: 0.9953 - loss: 0.0162\n",
      "Epoch 9/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1006s\u001b[0m 32ms/step - accuracy: 0.9954 - loss: 0.0156\n",
      "Epoch 10/10\n",
      "\u001b[1m31366/31366\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1010s\u001b[0m 32ms/step - accuracy: 0.9957 - loss: 0.0147\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x78c0fdd2ff10>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "It will take a long time for the entirety of this function to run. However, you can always stop execution after short durations to evaluate how the code is performing. The result in the bottommost cell is after training on only 816 out of a potential 313660 batches.\n",
    "The dataset has to be shuffled between each time you call this cell to avoid running the model only on the first few input-target pairs multiple times, which may cause you to think the model is performing better than it actually is.\n",
    "If the loss is consistently than 1 even at the start, or accuracy is very high at the start, be wary. You might want to shuffle the dataset and execute again\n",
    "'''\n",
    "dataset= tf.data.Dataset.from_tensor_slices((inputs, targets))\n",
    "dataset=dataset.shuffle(1000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ENgxJspKP7iR"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_index, num_generate=1):\n",
    "    '''\n",
    "        This function will generate text for num_generate characters, starting from start_index+batch_size.\n",
    "    '''\n",
    "    input_sequence = train_data[start_index:start_index + block_size]\n",
    "    generated_text = decode(input_sequence)\n",
    "    exact_sequence = decode(input_sequence)\n",
    "    for i in range(num_generate):\n",
    "        input_eval = tf.convert_to_tensor([input_sequence], dtype=tf.int32)\n",
    "        predictions = model.predict(input_eval, verbose = False)\n",
    "        probabilities = tf.nn.softmax(predictions[0, -1]).numpy()\n",
    "        next_token = np.random.choice(len(probabilities), p=probabilities)\n",
    "        input_sequence += [next_token]\n",
    "        input_sequence = input_sequence[1:]\n",
    "        exact_sequence += decode([np.argmax(probabilities)])\n",
    "        generated_text += decode([next_token])\n",
    "\n",
    "    return generated_text, exact_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "kKT7iVN1P7iR"
   },
   "outputs": [],
   "source": [
    "text_generated, _ = generate_text(model, start_index=0, num_generate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHp1E_Lw7aYy",
    "outputId": "ccaf921f-cf33-4d98-a16d-e60f683b0f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: Before we proceed any further, hear me speak.  All: Speak, speak.  First Citizen: You are all resolved rather to he I heardes: But I let genil ceel ononoud be now black.  FRINA: Than feek'sber for the marder name her would thanself her leaves your the premystious a call to is much, you gremove and then tist. Gith so I all That se. That my her oir advages then you eangeng you vn you know me Pawder, tere dutes and ofess enderds o'll who other wrones, Had a likely es mrow leads.  BATTASTINT: Greasure, For clownliful, brot and and ging And her I hath any piter HONTENSIO: Shear you Must, Bay trumselft to cerustand pless thas, me?  TRANINA: ? Garrot not, that alvens I have when, the die: his stumber off While me.  HORTET: And sweers, I with sill her: And nongen. wrome, To hast.  GREMIO: Areet will mine? Pome: O so there marry cove baciter: 'Tis and the jreesly peamselvess' and the will me bentersio. Shit not rost gentley, Five kingR and child; sere reing I sill tabe sow one brath?  TRANIO: To self yakent you brater one lovess will it good mystress sigh. whRth det to well not mighter never penter welce \n"
     ]
    }
   ],
   "source": [
    "print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdmKX5ju8QZr",
    "outputId": "1b9f0f5f-7c21-430c-f497-cdbc9c3ac0ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen: Before we proceed any further, hear me speak.  All: Speak, speak.  First Citizen: You are all resolved rather to you be the twettleman, here husber feir Grovest corting sirreling, criances.  KATHANIO: I the sfew that do for I with for And so her life him A shall may, Thy dnew have so know. Pairunast too bell, you a brisove her not of you qirong helling.  SSCMANTISTIO: I hath in the gevencue, fith then not of I be how is face so As me inton your conder with treasure to foss: Bot where for scegent bhosedy and her.  FUMENA: Kelpruin, Thus spechess loves mean the come's 'te will and time trusire And night, the for and Sawiss?  TRUMIO: I love they, not feely, As for a who ray that sly she mainess, That keqt.  PETRUCIO: You pence. Where's all come you me.  BATNABTHT: Take' to to lest offerces you can day be man a bore Begt I for nell and here one we such you be rogedo, I have your retunging you deed moud of my virefortand is insweet; Fike, your fieveds cerustalare's that I'll me is shall clatyse. What tot her where me the men man can she dever youngered, Tlay well, be pelce her her we rest up to birld kinglefore her be wrist, Mayne Bie held caure stand it horeous that I couse intoons may's have not you bream.  GREMANHITIO: And here? you know'st child of you word but and all such of you reming conter' As sckersio, friend my not your freasuren henging; her and made. when tan's keepell to undelf trees and yeg, is talking neen genne. Then of think so schier you, or honour good the wenc, vore forcettrne to the mist.  FRIARTIT: The beft: I know a sigell, in you'll this Hordds madress in this her him derack'st timen dech, I word statour Tell and and the fromindty if well for sweet To I sity come Somess where ouse.  BATNA: 'twere loved of my lord.  DSSCA: Whic he'll. meen me gitise  MARINTA: Their you grink they, ole I loves us meance well me my bell, I shis feecend fan: she pingoy, There helless see's thank Pelly As leaths That would of that enstate's and that in come stank you donguress.  GRTMIO: Hell bast spand, Befemeciond fot sids, In I dast word. Horteter and my dive all I wome?  TANTINA: Mess love? as am? in the sid: And shieve.  SAMINTIO: Have beat all be younge.  BI: Fifess of tis well as man here mower, Be withave of a bothst. Here a feart thourself  TRANIO: Belast aur me? Acced heardly say dang were is pomed shall so have sister by the sincan, for bimst if acce.  PETRUCNIZ: I hell; shall beepured.  DUKE VINCENTIO: You mest precute my cablail heard, Good spect ford whell in in a liest.  GREYCEBBINA:  USCONTESS: Dist! not Bidont me advage! Then stet not her ware my rene shall my brother no what her we as.  PETRUCIO: Why prise wislive.  WANANTISTINA: Hort bear befeles you when fith.  PETRUCIA: I well you wish you goods?  TRANIO: By the and the hearter.  BIATARTISADS: Tere less for muster of the she we well. Shy MINCA: Bore of why qinus, stead?  BANTISTRA:  DAKCISTIO: I, I grits: If conture you? The what cons gitold my what you not mazeld the tings upend you hall know old Promg'd to bet is she choing to is'lo for breth is no?  PREMANIO: The pay come.' and sister, for my lord: Put be would, let for to lead me and dcher need, I shido womes: May,  PETRUTIO: Now, will be heave, this his bleck scharg offing wome, such didon were is me let will be knears streil, fee is, you sighnoss but last a derido, and by wopgunges do, sack you chere of Ver of have is, a qoeth, And men of thoseives, happer him I thee her thou sechancer.  TRANIO: HORTENSTA: She as the you hanger me keem not to stawt come borsicolo?  SAMINTEBSIO: The canes be and thee me not not I she endere?  PETRUCHIO: Yene in the besss! And anots pittless, you for bobefile to die hell being such if you hath arother her did, let head hell's merIss to backing'd Boothing, I have trince: If it what lovenger?  TRANIO: And Ro, In one occess sirrely ristred Fngen, pire Ones be you, be well to choonan, thy her with?  PETRUCHINA: If EFTRUCNINTESTINA: She lacker-never pogsing wwad, I veck the herve I so lorder, Tell You seept conour as in carue.  PETRANIO: I dow, you lat gaing the might seepucy dounds, not\n"
     ]
    }
   ],
   "source": [
    "text_generated_2, _ = generate_text(model, start_index=0, num_generate=4000)\n",
    "print(text_generated_2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
